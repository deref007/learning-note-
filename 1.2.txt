<<<<<<< HEAD
字典学习（Dictionary Learning,KSVD)详解
字典学习也是一种数据降维的方法
字典学习思想：
字典是前辈们学习总结的精华，当我们需要学习新的知识的时候，不必学习前人学过的知识，我们可以参考前人总结的字典，通过查阅字典，我们可以大致学会到这些知识。
目的：字典尽可能要求全面，查字典的过程简洁，迅速，准确

稀疏模型：稀疏模型将大量的冗余变量去除，只保留与响应变量最相关的解释变量，简化了模型
的同时却保留了数据集中最重要的信息，有效解决了高维数据集建模中的诸多问题。

抽象成数学问题：
数学符号表示：
“以前的知识”，称之为原始样本，用矩阵Y来表示；
“字典”称之为字典矩阵，用D表示，字典中的词条，称之为原子（atom），用列向量dk表示
“查字典的方法”称之为稀疏矩阵，用X
“查字典的过程”可以用矩阵的乘法，用DX

数学语言描述：
字典学习的主要思想：利用K个原子dk的字典矩阵D（m*k）去稀疏线性表示原始样本Y（m*n:其中
m表示样本数，n表示样本的属性），即有Y=DX(理想情况），其中X为稀疏矩阵。可以将上述问题
用数学语言描述为如下的优化问题：
min||Y-DX||f  st. ||x||<=T或min||x||，st min||Y-DX|| <e

上式中X为稀疏编码的矩阵，xi为该矩阵中的行向量，代表字典矩阵的系数。

求解问题：因为要最小化查完的字典与原始样本的误差，表示查字典的方式要尽可能简单。
因此X要尽可能稀疏。
可以利用拉格朗日乘子法将其转化为无约束优化问题：
min||Y-DX|| + k||x||
因此这里有两个优化变量D，X。为解决这个优化问题，一般是固定其中一个优化，优化另一个变量，如此交替进行。其中稀疏矩阵X可以利用已有经典算法求解如Lasso OMP。

如何更新字典D：
假设X是已知的，于是逐列更新字典。
||Y-DX||=||Y-dx||=||(Y-式子）-dkxT
Y-式子=残差
此时优化问题可以描述为min（残差）
因此我们需要求出最优的dk，xT，这是一个最小二乘问题。可以利用最小二乘的方法求解，或者可以利用SVD进行求解，这列利用SVD的方式求解出两个优化变量。

SVD（奇异值分解）：
一、特征值分解
实对称矩阵：A是一个m*m矩阵 A=Q（lamda）QT ，lamda称为特征值
二、奇异值分解，一般矩阵的特征分解

对于一个实数矩阵（m*n）A。目的是想要把它分解成如下的形式
A=U（lamda）VT
U和V均为单位正交阵，U称为左奇异矩阵，V称为右奇异矩阵，lamda仅在主对角线上有值。





=======
Google Colab的使用教程

Google是一个免费的云服务，能支持免费的GPU

一、在Google Drive上创建文件夹
把文件连接到colab上，就可以进行编程序了

测试在前三个条件满足下的dup数量：
activemq：总:
derby：总：11036 dup：1618-3=1615
camel:  总：52713 duP:8306-2=8304
mahout:总：4629 dup：56


仔细分析detector这个类

监测函数为retrieve_duolicate_commits(),这个函数调用detector类的retrieve（）函数：
这个函数定义了两个列表：分别是duplicate_commit()和candidate_commits()
匹配的函数就是输入commit_id2,是否满足check_duplicate_commit()函数

也就是说明commit_id2是变量
因此函数check_diff()的变量就是必须commit_id2

def check_same_diff(self,commit_id2):
diff1=diff_git（log_index,gitlogs）#与类相连接
判断diff1是否等于diff2
diff2=diff_git(log_index,gitlogs)

还有一个方向就是把diff_git（）函数的变量变成commit_id2:
>>>>>>> learning_note



